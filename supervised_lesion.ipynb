{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, warnings, torch, multiprocessing, skimage, scipy, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "from prodigyopt import Prodigy\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, Dataset\n",
    "from model_extractors import resnet50_img_extractor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from masking_network import resnet50_trained_extractor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import IntegratedGradients\n",
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from torchray.attribution.extremal_perturbation import extremal_perturbation, contrastive_reward\n",
    "from quantus.metrics.faithfulness.faithfulness_estimate import FaithfulnessEstimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to the dataset\n",
    "base_path = \"data/HAM10000\"\n",
    "metadata_path = os.path.join(base_path, \"HAM10000_metadata.csv\")\n",
    "# Check if CUDA is available and set the device to GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "# Read metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "label_encoder = LabelEncoder()\n",
    "metadata['encoded_dx'] = label_encoder.fit_transform(metadata['dx'])\n",
    "class_counts = metadata['dx'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "\n",
    "# Assuming 'dx' is your target variable and it's encoded as integers\n",
    "class_labels = np.unique(metadata['encoded_dx'])  # Unique classes\n",
    "weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=metadata['encoded_dx'].values)\n",
    "class_weights = {label: weight for label, weight in zip(class_labels, weights)}\n",
    "\n",
    "print(class_weights)\n",
    "# Convert class weights to a tensor\n",
    "weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float)\n",
    "\n",
    "# Move to the same device as your model and inputs\n",
    "weights_tensor = weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkinCancerDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_img_dir, transform=None, skin_frame=None):\n",
    "        self.skin_frame = skin_frame if skin_frame is not None else pd.read_csv(csv_file)\n",
    "        self.base_img_dir = base_img_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = len(self.skin_frame['encoded_dx'].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skin_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.skin_frame.iloc[idx]['image_id']\n",
    "        img_path = os.path.join(self.base_img_dir,\"HAM_10000_images\" ,f\"{img_id}.jpg\")\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        label = self.skin_frame.iloc[idx]['encoded_dx']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label, self.skin_frame.iloc[idx]['image_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "     transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    " ])\n",
    "\n",
    "\n",
    "dataset = SkinCancerDataset(csv_file=metadata_path, base_img_dir=base_path, transform=transform, skin_frame=metadata)\n",
    "\n",
    "# Splitting dataset into train and test\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "random.seed(42)\n",
    "evaluation_idx = np.unique(random.sample(val_idx,1000))\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "evaluation_dataset = Subset(dataset, evaluation_idx)\n",
    "len(train_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_model(pl.LightningModule):\n",
    "    def __init__(self, model_string = \"resnet50\",):\n",
    "        super().__init__()\n",
    "        self.model = create_model(model_string, pretrained=True, num_classes=7,in_chans= 3)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Prodigy(self.parameters(), lr=1, weight_decay=1e-4, )\n",
    "        return optimizer\n",
    "model = classifier_model(\"resnet50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "BATCH = 32\n",
    "NUM_WORKERS = multiprocessing.cpu_count() - 2\n",
    "EPOCHS = 10\n",
    "MIXED_PRECISION = False\n",
    "DETERMINISTIC = False\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\", \n",
    "        precision=16 if MIXED_PRECISION else 32,\n",
    "        default_root_dir=\"classifier_lesion_resnet50_logs\",\n",
    "        accumulate_grad_batches=int(32/BATCH,),\n",
    "        deterministic=DETERMINISTIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.07886904761904762\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, test_loader)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "accs = []\n",
    "for batch in test_loader:\n",
    "    x, y , _  = batch\n",
    "    logits = (model(x.cuda())).detach().cpu()\n",
    "    acc = (logits.argmax(1) == y).float().mean()\n",
    "    accs.append(acc.item())\n",
    "model.cpu()\n",
    "print(\"Accuracy: \", sum(accs) / len(accs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 32\n",
    "NUM_WORKERS = multiprocessing.cpu_count() - 2\n",
    "EPOCHS = 5\n",
    "MIXED_PRECISION = False\n",
    "DETERMINISTIC = False\n",
    "CONTRASTIVE = False\n",
    "NOISE_MASK = False\n",
    "INVERSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extractor =  resnet50_img_extractor(model.model)\n",
    "masked_model = resnet50_trained_extractor(extractor, EPOCHS, batch_size=BATCH, lr = 1, \n",
    "                                           center = False, partition = 1, \n",
    "                                           noise_mask = NOISE_MASK, constrastive = CONTRASTIVE, inverse=INVERSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
    "trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\", \n",
    "        precision=16 if MIXED_PRECISION else 32,\n",
    "        default_root_dir=\"masking_supervised_lesion_logs\",\n",
    "        accumulate_grad_batches=int(128/BATCH,),\n",
    "        deterministic=DETERMINISTIC\n",
    ")\n",
    "\n",
    "trainer.fit(masked_model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "masked_model.eval()\n",
    "masked_model = masked_model.to(\"cuda\")\n",
    "nem_path = \"result/supervised/skin_lesion/nem_inv/\"\n",
    "os.makedirs(nem_path,exist_ok=True)\n",
    "execution_times = []\n",
    "for (img, y, name) in evaluation_dataset:\n",
    "    name = name.split(\"/\")[-1] + \".png\"\n",
    "    img = img.unsqueeze(0).to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    attr, _  = masked_model(img)\n",
    "    execution_times.append(time.time()-start_time)\n",
    "    x = img.squeeze().permute(1,2,0).cpu().detach().numpy()\n",
    "    x = ((x - x.min()) / (x.max() - x.min()))*255\n",
    "    attr = attr.cpu().squeeze().detach().numpy()\n",
    "    attr = (((attr - attr.min()) / (attr.max() - attr.min()))*255).astype(np.uint8)\n",
    "    attr = 255 - attr\n",
    "\n",
    "\n",
    "\n",
    "    Image.fromarray(attr).save(f\"{nem_path}{name}\")\n",
    "\n",
    "print(\"Average execution time: \", np.array(execution_times).sum()/len(execution_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = GradCAMPlusPlus(model, [model.model.layer4[-1]])   \n",
    "path = \"result/supervised/skin_lesion/gradcam/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "execution_times = []\n",
    "for (img,target, name) in evaluation_dataset:\n",
    "    name = name.split(\"/\")[-1] + \".png\"\n",
    "    start_time = time.time()\n",
    "    attr = method(input_tensor=img.unsqueeze(0).cuda(), targets=[ClassifierOutputTarget(target)]).squeeze()\n",
    "    execution_times.append(time.time()-start_time)\n",
    "    x = img.permute(1,2,0).cpu().detach().numpy()\n",
    "    x = (((x - x.min()) / (x.max() - x.min()))*255).astype(np.uint8)\n",
    "    \n",
    "    attr = (((attr - attr.min()) / (attr.max() - attr.min()))*255).astype(np.uint8)\n",
    "    Image.fromarray(attr).save(f\"{path}{name}\")\n",
    "print(\"Average execution time: \", np.array(execution_times).sum()/len(execution_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "method = GradientShap(model)\n",
    "baseline = torch.zeros((1, 3, 224, 224)).cuda()\n",
    "path = \"result/supervised/skin_lesion/grad_shap/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "execution_times = []\n",
    "for (img,target, name) in evaluation_dataset:\n",
    "    name = name.split(\"/\")[-1] + \".png\"\n",
    "    start_time = time.time()\n",
    "    attr = method.attribute(img.unsqueeze(0).cuda(),baselines=baseline,target= torch.tensor(target.argmax())).squeeze()\n",
    "    execution_times.append(time.time()-start_time)\n",
    "    x = img.permute(1,2,0).cpu().detach().numpy()\n",
    "    x = (((x - x.min()) / (x.max() - x.min()))*255).astype(np.uint8)\n",
    "    attr = torch.abs(attr).sum(0).cpu().numpy()\n",
    "    attr = (((attr - attr.min()) / (attr.max() - attr.min()))*255).astype(np.uint8)\n",
    "    Image.fromarray(attr).save(f\"{path}{name}\")\n",
    "print(\"Average execution time: \", np.array(execution_times).sum()/len(execution_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21134/2956624220.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attr = method.attribute(img.unsqueeze(0).cuda(),baselines=baseline,target= torch.tensor(target.argmax())).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time:  0.04806521725654602\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "method = IntegratedGradients(model)\n",
    "baseline = torch.zeros((1, 3, 224, 224)).cuda()\n",
    "path = \"result/supervised/skin_lesion/integrated_gradients/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "execution_times = []\n",
    "for (img,target, name) in evaluation_dataset:\n",
    "    name = name.split(\"/\")[-1] + \".png\"\n",
    "    start_time = time.time()\n",
    "    attr = method.attribute(img.unsqueeze(0).cuda(),baselines=baseline,target= torch.tensor(target.argmax())).squeeze()\n",
    "    execution_times.append(time.time()-start_time)\n",
    "    x = img.permute(1,2,0).cpu().detach().numpy()\n",
    "    x = (((x - x.min()) / (x.max() - x.min()))*255).astype(np.uint8)\n",
    "    attr = torch.abs(attr).sum(0).cpu().numpy()\n",
    "    attr = (((attr - attr.min()) / (attr.max() - attr.min()))*255).astype(np.uint8)\n",
    "    Image.fromarray(attr).save(f\"{path}{name}\")\n",
    "print(\"Average execution time: \", np.array(execution_times).sum()/len(execution_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth Pixel mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "class SmoothMask:\n",
    "    def __init__(self, area, model):\n",
    "        self.area = area \n",
    "        self.model = model\n",
    "    def __call__(self, x, pred):\n",
    "        mask, _ = extremal_perturbation(\n",
    "            self.model, x, pred,\n",
    "            reward_func=contrastive_reward,\n",
    "            debug=False,\n",
    "            areas=[self.area]\n",
    "        )\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = SmoothMask(model =model, area=0.05)\n",
    "path = \"result/supervised/skin_lesion/smooth_mask/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "execution_times = []\n",
    "for (img,target, name) in evaluation_dataset:\n",
    "    name = name.split(\"/\")[-1]+ \".png\"\n",
    "    start_time = time.time()\n",
    "    attr = method(img.unsqueeze(0).cuda(), int(target))\n",
    "    execution_times.append(time.time()-start_time)\n",
    "\n",
    "    x = img.permute(1,2,0).cpu().detach().numpy()\n",
    "    x = (((x - x.min()) / (x.max() - x.min()))*255).astype(np.uint8)\n",
    "    attr = attr.squeeze().cpu().detach().numpy()\n",
    "    attr = (((attr - attr.min()) / (attr.max() - attr.min()))*255).astype(np.uint8)\n",
    "    Image.fromarray(attr).save(f\"{path}{name}\")\n",
    "print(\"Average execution time: \", np.array(execution_times).sum()/len(execution_times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(metrics, metric_names,data):\n",
    "    res_path = \"result/supervised/skin_lesion/\"\n",
    "    mask_path = \"data/HAM10000/HAM10000_segmentations_lesion_tschandl/HAM10000_segmentations_lesion_tschandl/\"\n",
    "    resnet_50_smoothmask_path = f\"{res_path}smooth_mask/\"\n",
    "    resnet_50_gradcam_path = f\"{res_path}gradcam/\"\n",
    "    resnet_50_gradshape_path = f\"{res_path}grad_shap/\"\n",
    "    resnet_50_integrated_gradients_path = f\"{res_path}integrated_gradients/\"\n",
    "    resnet_50_nem_path = f\"{res_path}nem_inv/\"\n",
    "\n",
    "    for metric_func,metric_name in zip(metrics,metric_names):    \n",
    "        resnet_50_smoothmask_res = metric_func(mask_path=mask_path, explanation_path=resnet_50_smoothmask_path, samples=data)\n",
    "        resnet_50_gradcam_res = metric_func(mask_path=mask_path, explanation_path=resnet_50_gradcam_path, samples=data)\n",
    "        resnet_50_gradshape_res = metric_func(mask_path=mask_path, explanation_path=resnet_50_gradshape_path, samples=data)\n",
    "        resnet_50_integrated_gradients_res = metric_func(mask_path=mask_path, explanation_path=resnet_50_integrated_gradients_path, samples=data)\n",
    "        resnet_50_nem_res = metric_func(mask_path=mask_path, explanation_path=resnet_50_nem_path, samples=data)\n",
    "\n",
    "        print(f\"\"\"\n",
    "        {metric_name}:\n",
    "        resnet_50_smoothmask:     {resnet_50_smoothmask_res}\n",
    "        resnet_50_gradcam:        {resnet_50_gradcam_res}\n",
    "        resnet_50_gradshape:      {resnet_50_gradshape_res}\n",
    "        resnet_50_integrated_gradients: {resnet_50_integrated_gradients_res}\n",
    "        resnet_50_nem:            {resnet_50_nem_res}\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_rank( mask_path, explanation_path, samples, uncertainty=False):\n",
    "    rank_accuracy = 0\n",
    "    samps = 0\n",
    "    for _, _, img_name in samples:\n",
    "        s = (np.array(Image.open(mask_path + img_name + \"_segmentation.png\" ).resize((224,224))) > 0).astype(np.uint8)\n",
    "        a =  np.array(Image.open(explanation_path +  img_name + \".png\"))/255\n",
    "        # Prepare shapes.\n",
    "        a = a.flatten()\n",
    "        s = np.where(s.flatten().astype(bool))[0]\n",
    "        # Size of the ground truth mask.\n",
    "        k = len(s)\n",
    "        # Sort in descending order.\n",
    "        a_sorted = np.argsort(a)[-int(k) :]\n",
    "        # Calculate hits.\n",
    "        hits = len(np.intersect1d(s, a_sorted))\n",
    "        if hits != 0:\n",
    "            rank_accuracy += hits / float(k)\n",
    "        else:\n",
    "            rank_accuracy += 0.0\n",
    "        samps +=1 \n",
    "\n",
    "\n",
    "\n",
    "    return rank_accuracy/ samps\n",
    "\n",
    "def relevancy_mass(mask_path, explanation_path, samples, uncertainty=False):\n",
    "    mass_accuracy_total = 0\n",
    "    samps = 0\n",
    "    for _, _, img_name in samples:\n",
    "        s = (np.array(Image.open(mask_path + img_name + \"_segmentation.png\" ).resize((224,224))) > 0).astype(np.uint8)\n",
    "        a =  np.array(Image.open(explanation_path +  img_name + \".png\"))/255\n",
    "        # \n",
    "        a = a.flatten()\n",
    "        s = s.flatten().astype(bool)\n",
    "        # Compute inside/outside ratio.\n",
    "        r_within = np.sum(a[s])\n",
    "        r_total = np.sum(a)\n",
    "        # Calculate mass accuracy.\n",
    "        mass_accuracy = r_within / r_total\n",
    "        mass_accuracy_total += mass_accuracy\n",
    "        samps +=1\n",
    "\n",
    "    return mass_accuracy_total/ samps\n",
    "     \n",
    "\n",
    "run_exp([\n",
    "     relevance_rank, relevancy_mass],\n",
    "        [\n",
    "     \"Relevance Rank\", \"Relevancy Mass\"],\n",
    "    evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity( mask_path, explanation_path, samples,uncertainty=False):\n",
    "    complexity = 0\n",
    "    \n",
    "    for _, _, img_name in samples:\n",
    "        a =  np.array(Image.open(explanation_path +  img_name + \".png\"))/255\n",
    "        # Prepare shapes.\n",
    "        newshape = np.prod(a.shape)\n",
    "        a = np.array(np.reshape(a, newshape), dtype=np.float64) / np.sum(np.abs(a))\n",
    "        complexity += scipy.stats.entropy(pk=a)   \n",
    "\n",
    "\n",
    "    return complexity/ len(samples)\n",
    "\n",
    "\n",
    "def sparseness( mask_path, explanation_path, samples,uncertainty=False):\n",
    "    complexity = 0\n",
    "    \n",
    "    for _, _, img_name in samples:\n",
    "        a =  np.array(Image.open(explanation_path +  img_name + \".png\"))/255\n",
    "        # Prepare shapes.\n",
    "        newshape = np.prod(a.shape)\n",
    "        a = np.array(np.reshape(a, newshape), dtype=np.float64)\n",
    "        a += 0.0000001\n",
    "        a = np.sort(a)\n",
    "        complexity += (np.sum((2 * np.arange(1, a.shape[0] + 1) - a.shape[0] - 1) * a)) / (\n",
    "            a.shape[0] * np.sum(a)\n",
    "        )\n",
    "\n",
    "    return complexity/ len(samples)\n",
    "\n",
    "\n",
    "run_exp([\n",
    "    complexity, \n",
    "    sparseness\n",
    "    ],\n",
    "        [\n",
    "    \"Complexity\", \n",
    "    \"Sparseness\"\n",
    "    ],\n",
    "    evaluation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfullness(mask_path, explanation_path, samples):\n",
    "    skips = 0\n",
    "    metric = FaithfulnessEstimate(features_in_step=224 * 4)\n",
    "    values = []\n",
    "    i = 0\n",
    "    for X,Y,img_name in samples:\n",
    "        i += 1\n",
    "        img_name = img_name.split(\"/\")[-1]\n",
    "        a =  np.array(Image.open(explanation_path +  img_name + \".png\"))/255\n",
    "        Y = Y.argmax().numpy()\n",
    "        try:\n",
    "            values +=   metric(model=model.cuda().eval(),\n",
    "                            x_batch=X.unsqueeze(0).numpy(), y_batch=np.expand_dims(Y.astype(np.uint8), axis=0),\n",
    "                              a_batch=np.expand_dims(a, axis=0),device=\"cuda\")\n",
    "        except:\n",
    "            values += [0]\n",
    "            skips += 1\n",
    "            continue\n",
    "    print(f\"Skipped {skips} images\") \n",
    "    return np.nanmean(values)\n",
    "\n",
    "    \n",
    "run_exp([faithfullness],\n",
    "        [\"Faithfullness\"],\n",
    "    evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MICCAI_COMPARE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
